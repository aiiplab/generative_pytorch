# VAE (Variational Auto Encoder)
## Introduction
**VAE(Variational AutoEncoder)** is a generative model that extends AutoEncoder **to probabilistically encode input data</br>
into latent space** and restore data from the encoded latent vector

## Method
### AutoEncoder
updating...
![ae](/assets/VAE/ae.png)

- AutoEncoder consists of an **Encoder that compresses input data and Decoder that expands and restores it**
- However, autoencoders cannot generate **because they do not represent the latent space as a probability distribution**
- [Reference](https://medium.com/data-science/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2)

### Variational AutoEncoder(VAE)
![vae](/assets/VAE/vae.png)

- VAE allows **the encoder of AutoEncoder to represent the latent space as a probability distribution**
- VAE can express the latent space as a probability distribution, so **it can generate new data
through the decoder from any $z\sim \mathcal{N}(0,I)$ sample**

### Loss function
![loss](/assets/VAE/loss.png)

- **Reconstruction Loss**: How similar is the restored $\hat{x}$ to the input $x$?
- **KL Divergence**: The distance between the probability distribution $q(z|x)$ generated by the encoder</br>
and the standard normal distribution $p(z)=\mathcal{N}(0,1)$

## Sampling
- VAE samples the latent vector $z$ from the **normal distribution $z\sim \mathcal{N}(\mu, \sigma^2)$**
- However, $z$ sampled from the standard normal distribution is random, so the inverse difference is not possible.

### Reparameterization Trick
![reparameterization](/assets/VAE/reparameterization.png)
- VAE ㄸncoder outputs **mean($\mu$) and variance($\sigma^2$)** of the probability distribution representing the latent space, and can be learned
- $\epsilon$ is a noise vector sampled from a standard normal distribution
- Transform the sampling process from probabilistic to deterministic by controlling the $z$ to be sampled through learnable mean and variance

## Using code
### 1. root
```
root
├─ src
│  └─ vae
│     ├─ model.py         # VAE's mode code
│     ├─ loss.py          # VAE's loss function code
│     ├─ train.py         # VAE's training code
│     ├─ practice.ipynb   # VAE's tutorial notebook
│     └─ README.md
```

### 2. train script 
```
python src/vae/train.py 
```
If you use CPU:
```
python src/vae/train.py --device="cpu"
```

### 3. notebook
practice.ipynb is available at [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/aiiplab/generative_pytorch/blob/main/src/vae/pratice.ipynb)



## Reference
- Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." 20 Dec. 2013
- https://kyujinpy.tistory.com/88
