# Diffusion Models

## List of Models
1) ```DDPM(Denoising Diffusion Probabilistic Models)```
2) ```DDIM (Denoising Diffusion Implicit Models)```
3) ```CFG(Classifier-Free Diffusion Guidance)```

## Diffusion Models
### Introduction
![diffusion main](/assets/Diffusion/diffusion_main.png)
<p style="display: flex; justify-content: center; align-items: center; gap: 10px;">
  <img src="/assets/Diffusion/diffusion_image.gif" width="250">
  <img src="/assets/Diffusion/diffusion_distribution.gif" width="250">
  <img src="/assets/Diffusion/diffusion_sample.gif" width="250">
</p>

**Diffusion Model** is a generative model that learns **forward process** that gradually covers data with Gaussian noise,
and **reverse process** that removes this noise in reverse to restore the original.

### Forward Process
![diffusion forward](/assets/Diffusion/diffusion_forward.png)

- $q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_t\text{I})$ $\longrightarrow$ $q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1})$
- The given image now follows a probability distribution with mean $\sqrt{1-\beta_t}x_{t-1}$ and variance $\beta_t \text{I}$ after 1 step.
- The process of adding noise while reducing the proportion of signal in the data.

### Reverse Process
![diffusion_reverse](/assets/Diffusion/diffusion_reverse.png)

- $p(x_t)=\mathcal{N}(x_T;0,\text{I})$
- $p_\theta(x_{t-1}|x_t)=\mathcal{N}(x_{t-1};\mu_\theta(x_t,t), \sigma^2\text{I})$
- Fix the variance of the data distribution of the reverse process and **predict the mean of the data distribution at each step**


### Method
#### Model Architecture
![diffusion_unet](/assets/Diffusion/diffusion_unet.png)

- The model uses the [U-Net](https://arxiv.org/abs/1505.04597) architecture
- Consists of ```Residual Blocks```, ```Attention Layers```, and ```Skip Connections```
- Embed timestep $t$ and apply element-wise sum to the input of each block
- Model takes $x_t$ as input and predicts the noise $\mu_\theta(x_t, t)$ that was added to $x_t$.



### Loss function
![loss](/assets/VAE/loss.png)

- **Reconstruction Loss**: How similar is the restored $\hat{x}$ to the input $x$?
- **KL Divergence**: The distance between the probability distribution $q(z|x)$ generated by the encoder</br>
and the standard normal distribution $p(z)=\mathcal{N}(0,1)$

## Sampling
- VAE samples the latent vector $z$ from the **normal distribution $z\sim \mathcal{N}(\mu, \sigma^2)$**
- However, $z$ sampled from the standard normal distribution is random, so the inverse difference is not possible.

### Reparameterization Trick
![reparameterization](/assets/VAE/reparameterization.png)
- VAE ㄸncoder outputs **mean($\mu$) and variance($\sigma^2$)** of the probability distribution representing the latent space, and can be learned
- $\epsilon$ is a noise vector sampled from a standard normal distribution
- Transform the sampling process from probabilistic to deterministic by controlling the $z$ to be sampled through learnable mean and variance

## Using code
### 1. root
```
root
├─ src
│  └─ vae
│     ├─ model.py         # VAE's mode code
│     ├─ loss.py          # VAE's loss function code
│     ├─ train.py         # VAE's training code
│     ├─ practice.ipynb   # VAE's tutorial notebook
│     └─ README.md
```

### 2. train script 
```
python src/vae/train.py 
```
If you use CPU:
```
python src/vae/train.py --device="cpu"
```

### 3. notebook
practice.ipynb is available at [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aiiplab/generative_pytorch/blob/main/src/vae/pratice.ipynb)

## Reference
- Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." 20 Dec. 2013
- https://kyujinpy.tistory.com/88
